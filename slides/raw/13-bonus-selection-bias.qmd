---
title: "Bonus: Colliders, selection bias, and loss to follow-up"
author: "Malcolm Barrett"
institute: "Stanford University"
format: "kakashi-revealjs"
---

```{r}
#| label: setup
#| include: false
options(
  tibble.max_extra_cols = 6, 
  tibble.width = 60
)

library(tidyverse)
library(broom)
library(causaldata)

set.seed(100111)
```

## Confounders and chains

```{r}
#| echo: false
library(ggdag)
ggdag_confounder_triangle(x_y_associated = TRUE) + 
  theme_dag() +
  expand_plot(expansion(.2), expansion(.2))
```

## Colliders

```{r}
#| echo: false
ggdag_collider_triangle() + 
  theme_dag() +
  expand_plot(expansion(.2), expansion(.2))
```

## Colliders

```{r}
#| echo: false
ggdag_adjust(
  collider_triangle(), 
  "m", 
  collider_lines = FALSE
) + 
  theme_dag() +
  theme(legend.position = "none") +
  expand_plot(expansion(.2), expansion(.2)) +
  ggokabeito::scale_color_okabe_ito()
```

## Let's prove it!

```{r}
set.seed(1234)
collider_data <- collider_triangle() |> 
  simulate_data(-.6)
```

## Let's prove it!

```{r}
collider_data
```

## Let's prove it!

```{r}
#| echo: false
fit_model <- function(fmla) {
  lm(fmla, data = collider_data) |> 
    broom::tidy(conf.int = TRUE) |> 
    dplyr::filter(term == "x") |> 
    dplyr::mutate(formula = as.character(fmla)[3]) |> 
    dplyr::select(formula, estimate, conf.low, conf.high)
}

bind_rows(
  fit_model(y ~ x),
  fit_model(y ~ x + m)
) |> 
  ggplot(aes(x = estimate, y = formula, xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 0, size = 1, color = "grey80") + 
  geom_pointrange(color = "steelblue", fatten = 3, size = 1) +
  theme_minimal(18) +
  labs(
    y = NULL,
    caption = "correct effect size: 0"
  )
```

## Loss to follow-up

```{r}
#| echo: false
l2fu <- dagify(follow_up ~ symptoms,
       symptoms ~ new_rx + dx_severity,
       cd4 ~ dx_severity,
       labels = c(
         follow_up = "Follow-Up",
         symptoms = "Symptoms",
         new_rx = "New HIV Drug",
         dx_severity = "Underyling \nHIV Severity",
         cd4 = "CD4 Count"
       ), exposure = "new_rx", outcome = "cd4")

l2fu |>
  ggdag_adjust("follow_up", layout = "mds", text = FALSE, collider_lines = FALSE) + geom_dag_text_repel(aes(label = label), color = "black", point.padding = 100, seed = 1) + 
  theme_dag() +
  theme(legend.position = "none") +
  expand_plot() +
  ggokabeito::scale_color_okabe_ito()
```

## Adjusting for selection bias {background-color="#23373B"}

1. Fit a probability of censoring model, e.g. *glm(censoring ~ predictors, family = binomial())*
2. Create weights using inverse probability strategy
3. Use weights in your causal model

## {background-color="#23373B" .large .center}

### We won't do it here, but you can include many types of weights in a given model. Just take their product, e.g. *multiply inverse propensity of treatment weights by inverse propensity of censoring weights*.

## *Your Turn*

### Work through Your Turns 1-3 in `13-bonus-selection-bias.qmd`

`r countdown::countdown(minutes = 10)`
